# -*- coding: utf-8 -*-
"""transphla-esm-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PC7_ZP32Xxa-Q9OZ3XjBBfQRUH09k7S2
"""

import math
from sklearn import metrics
from sklearn import preprocessing
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
import time
import datetime
import random
random.seed(1234)

# from scipy import interp
import numpy as np

import warnings
warnings.filterwarnings("ignore")

from collections import Counter
from functools import reduce
from tqdm import tqdm, trange
from copy import deepcopy

from sklearn.metrics import confusion_matrix
from sklearn.metrics import roc_auc_score, auc
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import classification_report
from sklearn.utils import class_weight
from transformers import EsmModel, EsmTokenizer



import os
import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data as Data
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

seed = 19961231
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.cuda.manual_seed_all(seed)
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True

base_dir = '/rsrch5/home/bcb/jku/TransPHLA-AOMP-master/'

hla_sequence = pd.read_csv(os.path.join(base_dir, 'TransPHLA-AOMP/common_hla_sequence.csv'))
unique_hla_sequences = hla_sequence['HLA_sequence'].unique()
hla_max_len = 36


esm_model_name = "/rsrch5/home/bcb/jku/TransPHLA-AOMP-master/Procedure Code/esm_model"
tokenizer = EsmTokenizer.from_pretrained(esm_model_name, local_files_only=True)
esm_model = EsmModel.from_pretrained(esm_model_name, local_files_only=True)

# Evaluation mode?
esm_model.eval()

def compute_and_save_hla_embeddings(hla_sequences, tokenizer, model, save_path):
    hla_embeddings = {}
    hla_attention_masks = {}

    for idx, hla_seq in enumerate(hla_sequences):
        tokens = tokenizer(hla_seq, padding='max_length', max_length=hla_max_len, truncation=True, return_tensors='pt')

        with torch.no_grad():
            outputs = model(**tokens, output_attentions=True)

        hla_embeddings[hla_seq] = outputs.last_hidden_state.cpu()
        hla_attention_masks[hla_seq] = tokens['attention_mask'].cpu()  # Store the attention mask

        if idx < 5:  # Print for the first few sequences to avoid too much output
            print(f"HLA sequence: {hla_seq}")
            print(f"Tokenized input IDs: {tokens['input_ids']}")
            print(f"Tokenized attention mask: {tokens['attention_mask']}")
            print(f"Output embedding shape: {outputs.last_hidden_state.shape}")
            print(f"Output embeddings: {outputs.last_hidden_state}")

    torch.save((hla_embeddings, hla_attention_masks), save_path)  # Save both embeddings and masks
    print(f"HLA embeddings and attention masks saved to {save_path}")


hla_embeddings_path = os.path.join(base_dir, '1024_hla_embeddings.pt')
compute_and_save_hla_embeddings(unique_hla_sequences, tokenizer, esm_model, hla_embeddings_path)

def load_hla_embeddings(load_path):
    return torch.load(load_path)

hla_embeddings, hla_attention_masks = load_hla_embeddings(hla_embeddings_path)

def make_data(data, num_samples=None):

    pep_inputs, hla_inputs, pep_attention_masks, hla_attention_masks_list, labels = [], [], [], [], []

    for idx, (pep, hla, label) in enumerate(zip(data.peptide, data.HLA_sequence, data.label)):
        # Tokenize sequences
       
        
        pep_tokens = tokenizer(pep, padding='max_length', max_length=pep_max_len, truncation=True, return_tensors='pt')

        if idx < 5:
            print(f"Original peptide sequence ({idx}): {pep}")
            print(f"Tokenized peptide sequence ({idx}): {pep_tokens['input_ids']}")

        # Generate embeddings using ESM model with attention weights
        with torch.no_grad():
            pep_outputs = esm_model(**pep_tokens, output_attentions=True)

        pep_embedding = pep_outputs.last_hidden_state
        hla_embedding = hla_embeddings[hla]
        hla_attention_mask = hla_attention_masks[hla]


        if idx < 5:  # Print for the first few sequences to avoid too much output
            print(f"Peptide Embedding Shape for {pep}: {pep_embedding.shape}")
            print(f"Peptide embeddings: {pep_embedding}")
            print(f"Peptide attention mask: {pep_tokens['attention_mask']}")
            print(f"HLA Embedding Shape for {hla}: {hla_embedding.shape}")
            print(f"HLA embeddings: {hla_embedding}")
            print(f"HLA Attention Mask: {hla_attention_mask}")


        pep_inputs.append(pep_embedding)
        hla_inputs.append(hla_embedding)
        pep_attention_masks.append(pep_tokens['attention_mask'])
        hla_attention_masks_list.append(hla_attention_mask)
        labels.append(label)

        # Debugging: Print index to track progress
        if (idx + 1) % 1000 == 0:
            print(f"Processed {idx + 1} sequences")

    pep_inputs = torch.cat(pep_inputs)
    hla_inputs = torch.cat(hla_inputs)
    pep_attention_masks = torch.cat(pep_attention_masks)
    hla_attention_masks_list = torch.cat(hla_attention_masks_list)
    labels = torch.LongTensor(labels)

    return pep_inputs, hla_inputs, pep_attention_masks, hla_attention_masks_list, labels

class MyDataSet(Data.Dataset):
    def __init__(self, pep_inputs, hla_inputs, pep_attention_masks, hla_attention_masks, labels):
        super(MyDataSet, self).__init__()
        self.pep_inputs = pep_inputs
        self.hla_inputs = hla_inputs
        self.pep_attention_masks = pep_attention_masks
        self.hla_attention_masks = hla_attention_masks
        self.labels = labels

    def __len__(self):
        return self.pep_inputs.shape[0]

    def __getitem__(self, idx):
        return self.pep_inputs[idx], self.hla_inputs[idx], self.pep_attention_masks[idx], self.hla_attention_masks[idx], self.labels[idx]

# def get_attn_pad_mask(seq_q, seq_k):
#     '''
#     seq_q: [batch_size, seq_len, d_model]
#     seq_k: [batch_size, seq_len, d_model]
#     seq_len could be src_len or it could be tgt_len
#     seq_len in seq_q and seq_len in seq_k maybe not equal
#     '''
#     batch_size, len_q, _ = seq_q.size()
#     batch_size, len_k, _ = seq_k.size()
#     # 1 is PAD token
#     pad_attn_mask = seq_k.data.eq(1).sum(dim=-1).unsqueeze(1)  # [batch_size, 1, len_k], False is masked
#     pad_attn_mask = pad_attn_mask.expand(batch_size, len_q, len_k)  # [batch_size, len_q, len_k]

#     return pad_attn_mask

class CoAttLayer(nn.Module):
    def __init__(
            self,
            hidden_size: int,
            num_attention_heads: float,
            attention_dropout_prob: float,
            attention_out_dropout_prob: float,
            ctxt_dim = None,
    ):
        super().__init__()

        self.use_cuda = use_cuda
        self.device = torch.device("cuda" if self.use_cuda else "cpu")
        if hidden_size % num_attention_heads != 0:
            raise ValueError(
                "The hidden size (%d) is not a multiple of the number of attention "
                "heads (%d)" % (hidden_size, num_attention_heads))
        self.hidden_size = hidden_size
        self.num_attention_heads = num_attention_heads
        self.attention_head_size = int(self.hidden_size / self.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        if ctxt_dim is None:
            ctxt_dim = self.hidden_size
        self.state = nn.Linear(self.hidden_size, self.all_head_size, bias=False)
        self.context = nn.Linear(ctxt_dim, self.all_head_size, bias=False)
        self.context_v = nn.Linear(ctxt_dim, self.all_head_size, bias=False)
        self.state_v = nn.Linear(self.hidden_size, self.all_head_size, bias=False)
        self.dropout1_1 = nn.Dropout(attention_dropout_prob)
        self.dropout1_2 = nn.Dropout(attention_out_dropout_prob)
        self.dropout2_1 = nn.Dropout(attention_dropout_prob)
        self.dropout2_2 = nn.Dropout(attention_out_dropout_prob)
        self.LayerNorm1 = nn.LayerNorm((hidden_size,), eps=1e-12, elementwise_affine=True)
        self.LayerNorm2 = nn.LayerNorm((hidden_size,), eps=1e-12, elementwise_affine=True)
    def transpose_for_scores(self, x, num_attention_heads, attention_head_size):
        new_x_shape = x.size()[:-1] + (num_attention_heads, attention_head_size)
        x = x.view(*new_x_shape)
        return x.permute(0, 2, 1, 3)
    def forward(
            self,
            hidden_states: torch.Tensor, hidden_states_attention_mask,
            context: torch.Tensor, context_attention_mask,
    ):
        #max_length = max(hidden_states_attention_mask.shape[1], context_attention_mask.shape[1])
        #hidden_states_attention_mask = self.generate_attention_mask(hidden_states_attention_mask, max_length).to(self.device)
        #context_attention_mask = self.generate_attention_mask(context_attention_mask, max_length).to(self.device)
        state_layer = self.state(hidden_states)
        context_layer = self.context(context)
        state_value_layer = self.state_v(hidden_states)
        context_value_layer = self.context_v(context) # [batch_size, seq_length, all_head_size]
        state_layer = self.transpose_for_scores(state_layer, self.num_attention_heads, self.attention_head_size) # [batch_size, head_size, seq_length, num_attention_heads]
        context_layer = self.transpose_for_scores(context_layer, self.num_attention_heads, self.attention_head_size)
        state_value_layer = self.transpose_for_scores(state_value_layer, self.num_attention_heads, self.attention_head_size)
        context_value_layer = self.transpose_for_scores(context_value_layer, self.num_attention_heads, self.attention_head_size)

        state_attention_scores = torch.matmul(state_layer, context_layer.transpose(-1, -2)) # [batch_size, num_attention_head, seq1_length, seq2_length]
        state_attention_scores = state_attention_scores / math.sqrt(self.attention_head_size)
        if hidden_states_attention_mask is not None:
            print("Applying Mask: Hidden States")
            hidden_states_attention_mask = hidden_states_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
            print(f"Hidden states attention mask: {hidden_states_attention_mask}")
            state_attention_scores = state_attention_scores.masked_fill(hidden_states_attention_mask == 0, -1e9)
            print(f"State attention scores after masking: {state_attention_scores}")

        state_attention_probs = nn.Softmax(dim=-1)(state_attention_scores)
        state_attention_probs = self.dropout1_1(state_attention_probs)
        context_out = torch.matmul(state_attention_probs, context_value_layer)
        context_out = context_out.permute(0, 2, 1, 3).contiguous()
        new_context_out_shape = context_out.size()[:-2] + (self.all_head_size,)
        context_out = context_out.view(*new_context_out_shape)
        context_out = self.dropout1_2(context_out)
        context_out = self.LayerNorm1(context_out + hidden_states)


        context_attention_scores = torch.matmul(context_layer, state_layer.transpose(-1, -2))
        context_attention_scores = context_attention_scores / math.sqrt(self.attention_head_size)

        if context_attention_mask is not None:
            print("Applying Mask: Context")
            context_attention_mask = context_attention_mask.unsqueeze(1).unsqueeze(2)  # [batch_size, 1, 1, seq_len]
            print(f"Context attention mask: {context_attention_mask}")
            context_attention_scores = context_attention_scores.masked_fill(context_attention_mask == 0, -1e9)
            print(f"Context attention scores after masking: {context_attention_scores}")


        context_attention_probs = nn.Softmax(dim=-1)(context_attention_scores)
        context_attention_probs = self.dropout2_1(context_attention_probs)
        state_out = torch.matmul(context_attention_probs, state_value_layer)
        state_out = state_out.permute(0, 2, 1, 3).contiguous()
        new_state_out_shape = state_out.size()[:-2] + (self.all_head_size,)
        state_out = state_out.view(*new_state_out_shape)
        state_out = self.dropout2_2(state_out)
        state_out = self.LayerNorm2(state_out + context)
        return state_out, context_out, state_attention_probs, context_attention_probs


    def generate_attention_mask(self, padding_mask, max_length):
        mask = padding_mask.clone().to(self.device)
        mask = mask.type(torch.float)
        mask = mask[:,:,0]
        mask = torch.cat((mask.squeeze(), torch.zeros(mask.size()[0], max_length-mask.size()[-1]).to(self.device)), dim=1)
        mask = mask.unsqueeze(1).unsqueeze(2)
        # (1.0 -> 0.0) for positions we want to attend and (0.0 -> -10000.0) for masked positions.
        # Since we are adding it to the raw scores before the softmax at attention_probs calculation, this is effectively the same as removing these entirely.
        mask = (1.0 - mask) * -10000.0
        return mask

class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.use_cuda = use_cuda
        device = torch.device("cuda" if self.use_cuda else "cpu")
        self.co_att_layer = CoAttLayer(hidden_size=d_model, num_attention_heads=n_heads, attention_dropout_prob=0.1, attention_out_dropout_prob=0.1, ctxt_dim=d_model).to(device)

    def forward(self, dec_inputs, enc_outputs, dec_attention_mask, enc_attention_mask):  # dec_inputs = enc_outputs (batch_size, peptide_hla_maxlen_sum, d_model)
        '''
        dec_inputs: [batch_size, tgt_len]
        enc_inputs: [batch_size, src_len]
        enc_outputs: [batch_size, src_len, d_model]
        '''
        _, dec_outputs, _, _ = self.co_att_layer(dec_inputs, dec_attention_mask, enc_outputs, enc_attention_mask)

        return dec_outputs

class Transformer(nn.Module):
    def __init__(self):
        super(Transformer, self).__init__()
        self.use_cuda = use_cuda
        device = torch.device("cuda" if use_cuda else "cpu")
        self.decoder = Decoder().to(device)
        self.tgt_len = tgt_len  # Define tgt_len as a class attribute
        self.d_model = d_model  # Define d_model as a class attribute
        self.print_shapes_flag = True
        # Update the projection layer to match the input size
        self.projection = nn.Sequential(
            nn.Linear(self.tgt_len*self.d_model, 256),  # Correct input size
            nn.ReLU(True),
            nn.BatchNorm1d(256),
            nn.Linear(256, 64),
            nn.ReLU(True),
            nn.Linear(64, 2)
        ).to(device)

    def forward(self, pep_inputs, hla_inputs, pep_attention_mask, hla_attention_mask):
        '''
        pep_inputs: [batch_size, pep_len, d_model]
        pep_self_attns: list of attention weights for peptide inputs
        hla_inputs: [batch_size, hla_len, d_model]
        hla_self_attns: list of attention weights for HLA inputs
        '''
        # Use the ESM outputs directly
        pep_enc_outputs = pep_inputs
        hla_enc_outputs = hla_inputs

        # Decode concatenated outputs
        # hla_enc_mask = get_attn_pad_mask(hla_inputs, pep_inputs)
        # pep_enc_mask = get_attn_pad_mask(pep_inputs, hla_inputs)

        if self.print_shapes_flag:
            print(f"pep_inputs shape: {pep_inputs.shape}")
            print(f"hla_inputs shape: {hla_inputs.shape}")
            print(f"pep_attention_mask shape: {pep_attention_mask.shape}")
            print(f"hla_attention_mask shape: {hla_attention_mask.shape}")

            self.print_shapes_flag = False

        dec_outputs = self.decoder(pep_enc_outputs, hla_enc_outputs, pep_attention_mask, hla_attention_mask)
        dec_outputs = dec_outputs.view(dec_outputs.shape[0], -1)  # Flatten [batch_size, tgt_len * d_model]

        dec_logits = self.projection(dec_outputs)  # dec_logits: [batch_size, tgt_len, tgt_vocab_size]
        return dec_logits.view(-1, dec_logits.size(-1))

def performances(y_true, y_pred, y_prob, print_ = True):

    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels = [0, 1]).ravel().tolist()
    accuracy = (tp+tn)/(tn+fp+fn+tp)
    try:
        mcc = ((tp*tn) - (fn*fp)) / np.sqrt(np.float64((tp+fn)*(tn+fp)*(tp+fp)*(tn+fn)))
    except:
        print('MCC Error: ', (tp+fn)*(tn+fp)*(tp+fp)*(tn+fn))
        mcc = np.nan
    sensitivity = tp/(tp+fn)
    specificity = tn/(tn+fp)

    try:
        recall = tp / (tp+fn)
    except:
        recall = np.nan

    try:
        precision = tp / (tp+fp)
    except:
        precision = np.nan

    try:
        f1 = 2*precision*recall / (precision+recall)
    except:
        f1 = np.nan

    roc_auc = roc_auc_score(y_true, y_prob)
    prec, reca, _ = precision_recall_curve(y_true, y_prob)
    aupr = auc(reca, prec)

    if print_:
        print('tn = {}, fp = {}, fn = {}, tp = {}'.format(tn, fp, fn, tp))
        print('y_pred: 0 = {} | 1 = {}'.format(Counter(y_pred)[0], Counter(y_pred)[1]))
        print('y_true: 0 = {} | 1 = {}'.format(Counter(y_true)[0], Counter(y_true)[1]))
        print('auc={:.4f}|sensitivity={:.4f}|specificity={:.4f}|acc={:.4f}|mcc={:.4f}'.format(roc_auc, sensitivity, specificity, accuracy, mcc))
        print('precision={:.4f}|recall={:.4f}|f1={:.4f}|aupr={:.4f}'.format(precision, recall, f1, aupr))

    return (roc_auc, accuracy, mcc, f1, sensitivity, specificity, precision, recall, aupr)

def transfer(y_prob, threshold = 0.5):
    return np.array([[0, 1][x > threshold] for x in y_prob])

f_mean = lambda l: sum(l)/len(l)

def performances_to_pd(performances_list):
    metrics_name = ['roc_auc', 'accuracy', 'mcc', 'f1', 'sensitivity', 'specificity', 'precision', 'recall', 'aupr']

    performances_pd = pd.DataFrame(performances_list, columns = metrics_name)
    performances_pd.loc['mean'] = performances_pd.mean(axis = 0)
    performances_pd.loc['std'] = performances_pd.std(axis = 0)

    return performances_pd

def train_step(model, train_loader, fold, epoch, epochs, use_cuda=True):
    device = torch.device("cuda" if use_cuda else "cpu")
    time_train_ep = 0
    model.train()
    y_true_train_list, y_prob_train_list = [], []
    loss_train_list, dec_attns_train_list = [], []

    for train_pep_inputs, train_hla_inputs, train_labels in tqdm(train_loader):
        train_pep_inputs, train_hla_inputs, train_labels = train_pep_inputs.to(device), train_hla_inputs.to(device), train_labels.to(device)

        pep_attention_mask = train_pep_inputs['attention_mask'].to(device)
        hla_attention_mask = train_hla_inputs['attention_mask'].to(device)

        # print(f"train_pep_inputs shape: {train_pep_inputs.shape}")
        # print(f"train_hla_inputs shape: {train_hla_inputs.shape}")
        # print(f"pep_attention_mask: {pep_attention_mask}")
        # print(f"hla_attention_mask shape: {hla_attention_mask}")

        t1 = time.time()
        train_outputs = model(train_pep_inputs, train_hla_inputs, pep_attention_mask, hla_attention_mask)
        train_loss = criterion(train_outputs, train_labels)
        time_train_ep += time.time() - t1

        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()

        y_true_train = train_labels.cpu().numpy()
        y_prob_train = nn.Softmax(dim=1)(train_outputs)[:, 1].cpu().detach().numpy()

        y_true_train_list.extend(y_true_train)
        y_prob_train_list.extend(y_prob_train)
        loss_train_list.append(train_loss)

    y_pred_train_list = transfer(y_prob_train_list, threshold)
    ys_train = (y_true_train_list, y_pred_train_list, y_prob_train_list)

    print(f'Fold-{fold}****Train (Ep avg): Epoch-{epoch}/{epochs} | Loss = {f_mean(loss_train_list):.4f} | Time = {time_train_ep:.4f} sec')
    metrics_train = performances(y_true_train_list, y_pred_train_list, y_prob_train_list, print_=True)

    return ys_train, loss_train_list, metrics_train, time_train_ep

def eval_step(model, val_loader, fold, epoch, epochs, use_cuda=True):
    device = torch.device("cuda" if use_cuda else "cpu")
    model.eval()
    with torch.no_grad():
        loss_val_list, dec_attns_val_list = [], []
        y_true_val_list, y_prob_val_list = [], []
        for val_pep_inputs, val_hla_inputs, val_labels in tqdm(val_loader):
            val_pep_inputs, val_hla_inputs, val_labels = val_pep_inputs.to(device), val_hla_inputs.to(device), val_labels.to(device)

            pep_attention_mask = val_pep_inputs['attention_mask'].to(device)
            hla_attention_mask = val_hla_inputs['attention_mask'].to(device)

            val_outputs = model(val_pep_inputs, val_hla_inputs, pep_attention_mask, hla_attention_mask)
            val_loss = criterion(val_outputs, val_labels)

            y_true_val = val_labels.cpu().numpy()
            y_prob_val = nn.Softmax(dim=1)(val_outputs)[:, 1].cpu().detach().numpy()

            y_true_val_list.extend(y_true_val)
            y_prob_val_list.extend(y_prob_val)
            loss_val_list.append(val_loss)

        y_pred_val_list = transfer(y_prob_val_list, threshold)
        ys_val = (y_true_val_list, y_pred_val_list, y_prob_val_list)

        print(f'Fold-{fold} ****Test  Epoch-{epoch}/{epochs}: Loss = {f_mean(loss_val_list):.6f}')
        metrics_val = performances(y_true_val_list, y_pred_val_list, y_prob_val_list, print_=True)
    return ys_val, loss_val_list, metrics_val

pep_max_len = 17 # peptide; enc_input max sequence length
hla_max_len = 36 # hla; dec_input(=dec_output) max sequence length
tgt_len = pep_max_len

# Transformer Parameters
d_model = 320  # Embedding Size
d_ff = 1280 # FeedForward dimension
d_k = d_v = 320  # dimension of K(=Q), V
n_layers = 1  # number of Encoder of Decoder Layer

batch_size = 1024
epochs = 50
threshold = 0.5

use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

def data_with_loader(type_='train', fold=None, batch_size=batch_size):
    if type_ != 'train' and type_ != 'val':
        data = pd.read_csv(os.path.join(base_dir, f'Dataset/{type_}_set.csv'), index_col=0)
    elif type_ == 'train':
        data = pd.read_csv(os.path.join(base_dir, f'Dataset/train_data_fold{fold}.csv'), index_col=0)
    elif type_ == 'val':
        data = pd.read_csv(os.path.join(base_dir, f'Dataset/val_data_fold{fold}.csv'), index_col=0)

    pep_inputs, hla_inputs, pep_attention_masks, hla_attention_masks, labels = make_data(data)

    dataset = MyDataSet(pep_inputs, hla_inputs, pep_attention_masks, hla_attention_masks, labels)
    loader = Data.DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)

    return data, pep_inputs, hla_inputs, pep_attention_masks, hla_attention_masks, labels, loader

independent_data, independent_pep_inputs, independent_hla_inputs, independent_pep_attention_masks, independent_hla_attention_masks, independent_labels, independent_loader = data_with_loader(type_ = 'independent', fold = None, batch_size = batch_size)
external_data, external_pep_inputs, external_hla_inputs, external_pep_attention_masks, external_hla_attention_masks, external_labels, external_loader = data_with_loader(type_ = 'external', fold = None, batch_size = batch_size)

# train_data, train_pep_inputs, train_hla_inputs, train_labels, train_loader = data_with_loader(type_ = 'train', fold = 0,  batch_size = batch_size)
# model = Transformer().to(device)

# model.forward(train_loader, fold, epoch, epochs, use_cuda)

# for n_heads in [1, 2, 3, 4, 5]:
n_heads = 8

ys_train_fold_dict, ys_val_fold_dict = {}, {}
train_fold_metrics_list, val_fold_metrics_list = [], []
independent_fold_metrics_list, external_fold_metrics_list, ys_independent_fold_dict, ys_external_fold_dict = [], [], {}, {}
attns_train_fold_dict, attns_val_fold_dict, attns_independent_fold_dict, attns_external_fold_dict = {}, {}, {}, {}
loss_train_fold_dict, loss_val_fold_dict, loss_independent_fold_dict, loss_external_fold_dict = {}, {}, {}, {}

for fold in range(0, 5):
    print('=====Fold-{}====='.format(fold))
    print('-----Generate data loader-----')
    train_data, train_pep_inputs, train_hla_inputs, train_pep_attention_masks, train_hla_attention_masks, train_labels, train_loader = data_with_loader(type_ = 'train', fold = fold,  batch_size = batch_size)
    val_data, val_pep_inputs, val_hla_inputs, val_pep_attention_masks, val_hla_attention_masks, val_labels, val_loader = data_with_loader(type_ = 'val', fold = fold,  batch_size = batch_size)
    print('Fold-{} Label info: Train = {} | Val = {}'.format(fold, Counter(train_data.label), Counter(val_data.label)))

    print('-----Compile model-----')
    model = Transformer().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr = 1e-3)#, momentum = 0.99)

    print('-----Train-----')
    dir_saver = './1024_model_cached_embeddings/original/'
    path_saver = './1024_model_cached_embeddings/original/model_layer{}_multihead{}_fold{}.pkl'.format(n_layers, n_heads, fold)
    print('dir_saver: ', dir_saver)
    print('path_saver: ', path_saver)

    metric_best, ep_best = 0, -1
    time_train = 0
    for epoch in range(1, epochs + 1):

        ys_train, loss_train_list, metrics_train, time_train_ep = train_step(model, train_loader, fold, epoch, epochs, use_cuda) # , dec_attns_train
        ys_val, loss_val_list, metrics_val = eval_step(model, val_loader, fold, epoch, epochs, use_cuda) #, dec_attns_val

        metrics_ep_avg = sum(metrics_val[:4])/4
        if metrics_ep_avg > metric_best:
            metric_best, ep_best = metrics_ep_avg, epoch
            if not os.path.exists(dir_saver):
                os.makedirs(dir_saver)
            print('****Saving model: Best epoch = {} | 5metrics_Best_avg = {:.4f}'.format(ep_best, metric_best))
            print('*****Path saver: ', path_saver)
            torch.save(model.eval().state_dict(), path_saver)

        time_train += time_train_ep

    print('-----Optimization Finished!-----')
    print('-----Evaluate Results-----')
    if ep_best >= 0:
        print('*****Path saver: ', path_saver)
        model.load_state_dict(torch.load(path_saver))
        model_eval = model.eval()

        ys_res_train, loss_res_train_list, metrics_res_train = eval_step(model_eval, train_loader, fold, ep_best, epochs, use_cuda) # , train_res_attns
        ys_res_val, loss_res_val_list, metrics_res_val = eval_step(model_eval, val_loader, fold, ep_best, epochs, use_cuda) # , val_res_attns
        ys_res_independent, loss_res_independent_list, metrics_res_independent = eval_step(model_eval, independent_loader, fold, ep_best, epochs, use_cuda) # , independent_res_attns
        ys_res_external, loss_res_external_list, metrics_res_external = eval_step(model_eval, external_loader, fold, ep_best, epochs, use_cuda) # , external_res_attns

        train_fold_metrics_list.append(metrics_res_train)
        val_fold_metrics_list.append(metrics_res_val)
        independent_fold_metrics_list.append(metrics_res_independent)
        external_fold_metrics_list.append(metrics_res_external)

        ys_train_fold_dict[fold], ys_val_fold_dict[fold], ys_independent_fold_dict[fold], ys_external_fold_dict[fold] = ys_res_train, ys_res_val, ys_res_independent, ys_res_external
#             attns_train_fold_dict[fold], attns_val_fold_dict[fold], attns_independent_fold_dict[fold], attns_external_fold_dict[fold] = train_res_attns, val_res_attns, independent_res_attns, external_res_attns
        loss_train_fold_dict[fold], loss_val_fold_dict[fold], loss_independent_fold_dict[fold], loss_external_fold_dict[fold] = loss_res_train_list, loss_res_val_list, loss_res_independent_list, loss_res_external_list

    print("Total training time: {:6.2f} sec".format(time_train))

print('****Independent set:')
print(performances_to_pd(independent_fold_metrics_list))
print('****External set:')
print(performances_to_pd(external_fold_metrics_list))
print('****Train set:')
print(performances_to_pd(train_fold_metrics_list))
print('****Val set:')
print(performances_to_pd(val_fold_metrics_list))

print(model.parameters)
